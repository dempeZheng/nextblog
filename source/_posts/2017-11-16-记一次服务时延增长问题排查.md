---
layout: post
title:  "记一次服务时延增长问题排查"
date:  2017-11-16 12:39:04
type: case
categories: [case]
keywords: 时延,优化，高并发
---


## 场景
线上服务高峰期间cpu300~400%飙高，系统使用内存正常（较低），cat监控到整个接口的耗时增长两倍多。
1）首先怀疑cpu飙高，cpu是系统成为系统瓶颈，故

sar命令查看到服务各个cpu使用值相对还是比较低，sys和user使用cpu也没毛病，cpu应该不是瓶颈。
```bash
sar -A  1 1
```

```console
Linux 2.6.32       11/14/2017      _x86_64_        (24 CPU)

06:52:10 PM     CPU      %usr     %nice      %sys   %iowait    %steal      %irq     %soft    %guest     %idle
06:52:11 PM     all      4.52      0.00      0.71      0.00      0.00      0.00      0.04      0.00     94.73
06:52:11 PM       0      1.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00     98.00
06:52:11 PM       1      1.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00     99.00
06:52:11 PM       2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    100.00
06:52:11 PM       3      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    100.00
06:52:11 PM       4      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    100.00
06:52:11 PM       5      0.00      0.00      0.99      0.00      0.00      0.00      0.00      0.00     99.01
06:52:11 PM       6     19.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00     80.00
06:52:11 PM       7      5.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00     94.00
06:52:11 PM       8      8.08      0.00      0.00      0.00      0.00      0.00      0.00      0.00     91.92
06:52:11 PM       9      7.07      0.00      0.00      0.00      0.00      0.00      0.00      0.00     92.93
06:52:11 PM      10      1.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00     99.00
06:52:11 PM      11     15.15      0.00      1.01      0.00      0.00      0.00      0.00      0.00     83.84
...
```
顺便也用脚本把当前耗用cpu较高的线程给print出来，也没发现什么异常。

2）因为是java进程，gc也是首要怀疑对象。
线上是有开启gc日志的，直接查看gc日志，发现gc日志没变化。
如下所示，gc日志一直固定在这里，没有任何输出。
当时误以为没有gc活动，对后来的排查带来了很多困惑。

```
Heap
 PSYoungGen      total 1340416K, used 940106K [0x000000076ab00000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1284096K, 71% used [0x000000076ab00000,0x00000007a2e1a438,0x00000007b9100000)
  from space 56320K, 34% used [0x00000007b9100000,0x00000007ba3f8468,0x00000007bc800000)
  to   space 54272K, 0% used [0x00000007bcb00000,0x00000007bcb00000,0x00000007c0000000)
 ParOldGen       total 2796544K, used 99854K [0x00000006c0000000, 0x000000076ab00000, 0x000000076ab00000)
  object space 2796544K, 3% used [0x00000006c0000000,0x00000006c6183988,0x000000076ab00000)
 Metaspace       used 188543K, capacity 305226K, committed 318036K, reserved 1331200K
  class space    used 15913K, capacity 23093K, committed 35736K, reserved 1048576K
```
3）因为gc日志没有明显活动，就忽视了有gc问题导致时延，开始怀疑是服务中某个依赖服务拖慢整个请求链。
仔细查看cat监控远程调用的响应，并未能发现明显的异常。

## 
排查完上面三步，其实已经有些困惑。一时找不到问题症结点，先扩容了。线上也难复现这个场景。

走了一圈弯路，又怀疑到gc上面来。然后通过下面命令查看 gc情况。

```
jstat -gc -pid -interval
```

大跌眼镜！ 竟然有gc，另外gc频率还比较频繁，一分钟有一次fullgc。

具体的gc日志没出来，好多gc的细节无从得知。

不过幸好，gc日志过了几天有突然出来了，这一点到现在也不解。
>
gc日志如下：

```console
25298.606: [Full GC (Metadata GC Threshold) [PSYoungGen: 38137K->0K(1342976K)] [ParOldGen: 98007K->97852K(2796544K)] 136145K->97852K(4139520K), [Metaspace: 258166K->88911K(1284096K)], 0.3885363 secs] [Times: user=2.22 sys=0.05, real=0.38 secs] 
25309.889: [GC (Allocation Failure) [PSYoungGen: 1294336K->7488K(1302016K)] 1392188K->105340K(4098560K), 0.0210329 secs] [Times: user=0.29 sys=0.01, real=0.02 secs] 
25321.252: [GC (Allocation Failure) [PSYoungGen: 1301824K->13757K(1347072K)] 1399676K->111610K(4143616K), 0.0276562 secs] [Times: user=0.42 sys=0.00, real=0.03 secs] 
25332.233: [GC (Allocation Failure) [PSYoungGen: 1308605K->20085K(1345536K)] 1406458K->117938K(4142080K), 0.0412415 secs] [Times: user=0.66 sys=0.00, real=0.04 secs] 
25342.077: [GC (Allocation Failure) [PSYoungGen: 1314933K->26152K(1350656K)] 1412786K->124005K(4147200K), 0.0556676 secs] [Times: user=0.92 sys=0.00, real=0.05 secs] 
25353.367: [GC (Allocation Failure) [PSYoungGen: 1328168K->32325K(1349120K)] 1426021K->130178K(4145664K), 0.0688141 secs] [Times: user=1.16 sys=0.00, real=0.07 secs] 
25360.830: [GC (Metadata GC Threshold) [PSYoungGen: 1105399K->38061K(1344000K)] 1203252K->135914K(4140544K), 0.0783975 secs] [Times: user=1.34 sys=0.00, real=0.08 secs] 
25360.909: [Full GC (Metadata GC Threshold) [PSYoungGen: 38061K->0K(1344000K)] [ParOldGen: 97852K->97904K(2796544K)] 135914K->97904K(4140544K), [Metaspace: 255138K->88936K(1284096K)], 0.3513680 secs] [Times: user=1.67 sys=0.05, real=0.35
```
乍一看到日志，感觉很匪夷所思：
为什么要fullgc，老年代的空间还是很大的，使用不到20%就触发fullgc，而且每次回收老年代也没回收到多少空间。

既然发生了fullgc,那么首先应该了解fullgc的触发条件


## Metasapce会触发fullgc？

于是去google Metaspace相关的信息。

>-XX:MetaspaceSize是分配给类元数据空间（以字节计）的初始大小(Oracle逻辑存储上的初始高水位，the initial high-water-mark )，此值为估计值。MetaspaceSize的值设置的过大会延长垃圾回收时间。垃圾回收过后，引起下一次垃圾回收的类元数据空间的大小可能会变大。

看到高水位线基本就可以确定服务的fullgc是由于Metaspace的超过高水位线导致的.

怎么解决?

首先想到,必然是调高XX:MetaspaceSize的值,这样至少可以降低fullgc的频率.

但是，为什么Metaspace会增长这么快？Metaspace的前生不是永久带吗，没道理会这么频繁增长！

重启服务，在启动前加上参数 `-verbose:class`，于是发现了大量的以下日志

```
[Loaded Serializer_1 from file:/data1/platform/platform_consume_service/platform_consume_service_201711081505_platform_consume_service_v0.11.4/lib/fastjson-1.2.7.jar]
[Loaded Serializer_1 from file:/data1/platform/platform_consume_service/platform_consume_service_201711081505_platform_consume_service_v0.11.4/lib/fastjson-1.2.7.jar]
[Loaded Serializer_1 from file:/data1/platform/platform_consume_service/platform_consume_service_201711081505_platform_consume_service_v0.11.4/lib/fastjson-1.2.7.jar]
[Loaded Serializer_1 from file:/data1/platform/platform_consume_service/platform_consume_service_201711081505_platform_consume_service_v0.11.4/lib/fastjson-1.2.7.jar]
[Loaded Serializer_1 from file:/data1/platform/platform_consume_service/platform_consume_service_201711081505_platform_consume_service_v0.11.4/lib/fastjson-1.2.7.jar]
```

fastjson-1.2.7有坑？不能适应jdk8？
google一下，发现fastjson会用到asm动态加载类，好吧 估计坑就在这里

运气比较好，一下子就搜到一个比较类似的issue
https://github.com/alibaba/fastjson/issues/431

>wenshao commented on 13 Apr 2016
不会的，估计是你每次都new SerializeConfig和new ParserConfig.
 @wenshao wenshao closed this on 13 Apr 2016

检查了一下代码，发现恰好，之前实现了一个toString的方法解决double类型转String科学计数法的问题

```java
public static String toString(Object obj) {
    SerializeConfig config = new SerializeConfig();
    config.put(Double.class, new DoubleSerializer());
    return JSON.toJSONString(obj, config, new SerializerFeature[0]);
}
```

尴尬了，坑找到了，把SerializeConfig变成静态，问题解决。

起初以为这个仅仅只是影响了gc，后来看了看SerializeConfig的实现，发现这个对象太重，new一个SerializeConfig要通过asm动态的创建一个序列化的类.class文件，设计到写文件，然后还要调用类加载器加载。重复构造简直是性能杀手。
优化后上线，应用层服务也再也没发现什么性能问题。

踩到这个坑一方面是自己的草率，出于对fastjson的api的信任，没有仔细api就想当然的使用了；

## 总结
虽然找问题过程比较曲折，但是能够多多遇见这种case还是很不错的。在整个排查的过程中巩固了好多知识点。
bug驱动进步。

-----
## 后记
还有一个遗留问题，如鲠在喉。
启动脚本明确加了输出gc日志的参数，`-XX:+PrintGCDetails -Xloggc:$LOG_DIR/gc.log` gc日志对应的文件也有，为什么有gc活动的时候，日志没有输出？？？
一直卡在这里，文件时间也是实时更新，why？
```
Heap
 PSYoungGen      total 1340416K, used 940106K [0x000000076ab00000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 1284096K, 71% used [0x000000076ab00000,0x00000007a2e1a438,0x00000007b9100000)
  from space 56320K, 34% used [0x00000007b9100000,0x00000007ba3f8468,0x00000007bc800000)
  to   space 54272K, 0% used [0x00000007bcb00000,0x00000007bcb00000,0x00000007c0000000)
 ParOldGen       total 2796544K, used 99854K [0x00000006c0000000, 0x000000076ab00000, 0x000000076ab00000)
  object space 2796544K, 3% used [0x00000006c0000000,0x00000006c6183988,0x000000076ab00000)
 Metaspace       used 188543K, capacity 305226K, committed 318036K, reserved 1331200K
  class space    used 15913K, capacity 23093K, committed 35736K, reserved 1048576K
```


--------------------
### gc日子很问题1119补充
至于gc日志为什么没有输出这个问题最近已经找出来了，其实并不是没有更新，而是在写gc日志的时候是采用覆盖的方式，之前的gc日志内容并未删除，tail 的方式查看只能看到最后几行，而gc日志很可能就是在gc.log的中间覆盖旧数据。
目前的解决办法比较粗暴，启动进程前将之前的gc日志删掉。
## 参考文档
http://calvin1978.blogcn.com/articles/latency.html